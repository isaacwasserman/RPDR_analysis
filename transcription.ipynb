{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-ffmpeg pyannote.audio python-dotenv git+https://github.com/openai/whisper.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from ffmpeg import FFmpeg\n",
    "import tqdm.auto as tqdm\n",
    "from pyannote.audio import Pipeline as PyannotePipeline\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "import dotenv\n",
    "import torch\n",
    "import torchaudio\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from IPython.display import Video\n",
    "import whisper\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_root = \"videos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract audio from videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"audio_tracks\", exist_ok=True)\n",
    "video_paths = glob.glob(f\"{video_root}/*/*\", recursive=True)\n",
    "for video_path in tqdm.tqdm(video_paths):\n",
    "    audio_path = os.path.join(\"audio_tracks\", \".\".join(os.path.basename(video_path).split(\".\")[:-1]) + \".mp3\")\n",
    "    if os.path.exists(audio_path):\n",
    "        continue\n",
    "    FFmpeg().input(video_path).output(audio_path).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization_pipeline = PyannotePipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\", use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    ").to(device)\n",
    "\n",
    "os.makedirs(\"diarizations\", exist_ok=True)\n",
    "audio_paths = glob.glob(\"audio_tracks/*.mp3\")\n",
    "for audio_path in tqdm.tqdm(audio_paths):\n",
    "    diarization_path = os.path.join(\"diarizations\", \".\".join(os.path.basename(audio_path).split(\".\")[:-1]) + \".pkl\")\n",
    "    if os.path.exists(diarization_path):\n",
    "        continue\n",
    "    with ProgressHook() as hook:\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        diarization = diarization_pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate}, hook=hook)\n",
    "        with open(diarization_path, \"wb\") as f:\n",
    "            pickle.dump(diarization, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify speakers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate speech samples for manual annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization_paths = glob.glob(\"diarizations/*.pkl\")\n",
    "num_clips = 4\n",
    "minimum_speaking_time = 50\n",
    "\n",
    "supercuts = []\n",
    "for diarization_path in diarization_paths:\n",
    "    with open(diarization_path, \"rb\") as f:\n",
    "        diarization = pickle.load(f)\n",
    "    video_id = os.path.basename(diarization_path).split(\".\")[0]\n",
    "    video_path = glob.glob(f\"{os.path.join('videos', video_id)}.*\")[0]\n",
    "    intervals_by_speaker = {}\n",
    "    for interval, turn, speaker in diarization.itertracks(yield_label=True):\n",
    "        if speaker not in intervals_by_speaker:\n",
    "            intervals_by_speaker[speaker] = []\n",
    "        intervals_by_speaker[speaker].append((interval.start, interval.end))\n",
    "    total_time_by_speaker = {\n",
    "        speaker: sum(interval[1] - interval[0] for interval in intervals) for speaker, intervals in intervals_by_speaker.items()\n",
    "    }\n",
    "    for speaker in tqdm.tqdm(diarization.labels(), desc=f\"Generating super cuts for each speaker in video {video_id}\"):\n",
    "        if total_time_by_speaker[speaker] < minimum_speaking_time:\n",
    "            continue\n",
    "        extracted_interval_paths = []\n",
    "        intervals_by_speaker[speaker] = sorted(\n",
    "            intervals_by_speaker[speaker], key=lambda interval: interval[1] - interval[0], reverse=True\n",
    "        )\n",
    "        for interval_index in range(num_clips):\n",
    "            interval_index = int(interval_index)\n",
    "            interval = intervals_by_speaker[speaker][interval_index]\n",
    "            middle = (interval[0] + interval[1]) / 2\n",
    "            os.makedirs(\"tmp\", exist_ok=True)\n",
    "            extract_interval(video_path, interval, f\"tmp/{video_id}_{speaker}_{interval_index:02d}.mp4\")\n",
    "            extracted_interval_paths.append(f\"tmp/{video_id}_{speaker}_{interval_index:02d}.mp4\")\n",
    "        if len(extracted_interval_paths) < num_clips:\n",
    "            extracted_interval_paths += [extracted_interval_paths[-1]] * (num_clips - len(extracted_interval_paths))\n",
    "        output_file = f\"tmp/supercuts/{video_id}_{speaker}.mp4\"\n",
    "        concatenate_videos(extracted_interval_paths, output_file)\n",
    "        supercuts.append((video_id, speaker, output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually annotate speech samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_speaker_identification_form(supercuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate supercut for each true speaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"diarization_mappings.json\", \"r\") as f:\n",
    "    diarization_mappings = json.load(f)\n",
    "\n",
    "intervals_by_speaker = {}\n",
    "\n",
    "diarization_paths = glob.glob(\"diarizations/*.pkl\")\n",
    "for diarization_path in diarization_paths:\n",
    "    with open(diarization_path, \"rb\") as f:\n",
    "        diarization = pickle.load(f)\n",
    "    video_id = os.path.basename(diarization_path).split(\".\")[0]\n",
    "    video_path = glob.glob(f\"{os.path.join('videos', video_id)}.*\")[0]\n",
    "    for interval, turn, speaker in diarization.itertracks(yield_label=True):\n",
    "        if speaker not in diarization_mappings[video_id]:\n",
    "            continue\n",
    "        speaker_name = diarization_mappings[video_id][speaker]\n",
    "        if speaker_name not in intervals_by_speaker:\n",
    "            intervals_by_speaker[speaker_name] = []\n",
    "        record = {\n",
    "            \"video_id\": video_id,\n",
    "            \"start\": interval.start,\n",
    "            \"end\": interval.end,\n",
    "        }\n",
    "        intervals_by_speaker[speaker_name].append(record)\n",
    "\n",
    "with open(\"intervals_by_speaker.json\", \"w\") as f:\n",
    "    json.dump(intervals_by_speaker, f)\n",
    "\n",
    "for speaker in intervals_by_speaker:\n",
    "    os.makedirs(f\"tmp/speaker_clips/{speaker}\", exist_ok=True)\n",
    "    for record in tqdm.tqdm(intervals_by_speaker[speaker], desc=f\"Generating supercut for {speaker}\"):\n",
    "        video_id = record[\"video_id\"]\n",
    "        video_path = glob.glob(f\"{os.path.join('videos', video_id)}.*\")[0]\n",
    "        interval = (record[\"start\"], record[\"end\"])\n",
    "        extract_interval(\n",
    "            video_path, interval, f\"tmp/speaker_clips/{speaker}/{video_id}_{interval[0]:.2f}_{interval[1]:.2f}.mp4\"\n",
    "        )\n",
    "    clip_paths = glob.glob(f\"tmp/speaker_clips/{speaker}/*.mp4\")\n",
    "    concatenate_videos(clip_paths, f\"supercuts/{speaker}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip audio from supercuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff6765caf8447e1be853a31e17e517f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shutil.rmtree(\"tmp\", ignore_errors=True)\n",
    "os.makedirs(\"tmp/supercut_audio\", exist_ok=True)\n",
    "supercut_video_paths = glob.glob(f\"supercuts/*\", recursive=True)\n",
    "for supercut_video_path in tqdm.tqdm(supercut_video_paths):\n",
    "    supercut_audio_path = os.path.join(\"tmp\", \"supercut_audio\", os.path.basename(supercut_video_path).split(\".\")[0] + \".mp3\")\n",
    "    if os.path.exists(supercut_audio_path):\n",
    "        continue\n",
    "    FFmpeg().input(supercut_video_path).output(supercut_audio_path).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ggml model base.en from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
      "Model base.en already exists. Skipping download.\n",
      "whisper_init_from_file_with_params_no_state: loading model from 'models/ggml-base.en.bin'\n",
      "whisper_init_from_file_with_params_no_state: failed to open 'models/ggml-base.en.bin'\n",
      "error: failed to initialize whisper context\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/whisper.cpp.git > /dev/null 2>&1\n",
    "!bash ./whisper.cpp/models/download-ggml-model.sh base.en\n",
    "!cd whisper.cpp && make -j10 > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6306d3aeca4eeeb42043295e5b3a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/supercut_audio/Akashia.mp3\n",
      "tmp/supercut_audio/Victoria Porkchop Parker.mp3\n",
      "tmp/supercut_audio/Shannel.mp3\n",
      "tmp/supercut_audio/RuPaul.mp3\n",
      "tmp/supercut_audio/Tammie Brown.mp3\n",
      "tmp/supercut_audio/Nina Flowers.mp3\n",
      "tmp/supercut_audio/BeBe Zahara Benet.mp3\n",
      "tmp/supercut_audio/Jade.mp3\n",
      "tmp/supercut_audio/Rebecca Glasscock.mp3\n",
      "tmp/supercut_audio/Ongina.mp3\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"speaker_transcripts\", exist_ok=True)\n",
    "results = []\n",
    "for supercut_audio_path in tqdm.tqdm(glob.glob(\"tmp/supercut_audio/*.mp3\")):\n",
    "    wav_path = os.path.join(\"tmp\", \"output.wav\")\n",
    "    output_path = os.path.join(\"speaker_transcripts\", os.path.basename(supercut_audio_path).split(\".\")[0] + \".txt\")\n",
    "    !ffmpeg -i \"{supercut_audio_path}\" -ar 16000 -ac 1 -c:a pcm_s16le {wav_path} -y > /dev/null 2>&1\n",
    "    !cd whisper.cpp && ./main -otxt -f \"{os.path.abspath(wav_path)}\" > /dev/null 2>&1\n",
    "    !mv \"{os.path.abspath(wav_path)}.txt\" \"{output_path}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
